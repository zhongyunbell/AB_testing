{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec940668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangz36/miniconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as ps\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e52ac5",
   "metadata": {},
   "source": [
    "# 2023 July A-B testing useful resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fadde8",
   "metadata": {},
   "source": [
    "# SESSION1: A/B testing data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9d3105",
   "metadata": {},
   "source": [
    "### STEP1: mixed assignemnt\n",
    "#### Whether same user_id assigned to both groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "109456ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_mixed_assignment(df):\n",
    "    # check the no. of users who have been assigned to multiple groups\n",
    "    df1 = df[['user_id', 'group']].groupby(['user_id']).nunique().reset_index()\n",
    "    df2 = df1[df1['group']>1]['user_id'].count()\n",
    "    print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c39fa4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mixed_assignment(df):\n",
    "    # remove user_id that have been assigned multiple groups\n",
    "    df1 = df[['user_id', 'group']].gorupby(['user_id']).nunique().reset_index()\n",
    "    df2 = pd.merge(df, df1, on=['user_id'], how='left')\n",
    "    return df2[df2['group_y']==1][['uer_id', 'time_stamp', 'group_x', 'anding_page', 'converted']]\\\n",
    ".rename(columns={'group_x', group})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4c91ffb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'remove_mixed_assignement' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data1 \u001b[38;5;241m=\u001b[39m \u001b[43mremove_mixed_assignement\u001b[49m(data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'remove_mixed_assignement' is not defined"
     ]
    }
   ],
   "source": [
    "data1 = remove_mixed_assignement(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0611722",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m check_mixed_assignment(\u001b[43mdata1\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data1' is not defined"
     ]
    }
   ],
   "source": [
    "check_mixed_assignment(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19532977",
   "metadata": {},
   "source": [
    "### STEP2: exposure bug\n",
    "#### wheather controlled got assigned \"new_page\", which treatment assigned \"old_page\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0adfa295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_exposure_bugs(df):\n",
    "    print(df[(df['group']=='control') & (df['landing_page']=='new_page')]['user_id'].count())\n",
    "    print(df[(df['group']=='treatment') & (df['landing_page']=='old_page')]['user_id'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80cc3207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_exposure_bugs(df):\n",
    "    df1 = df[(df['group']=='control') & (df['landing_page']=='new_page')][['user_id', 'group']]\n",
    "    # identify control group users exposed to treament\n",
    "    df2 = df[(df['group']=='treatment') & (df['landing_page']=='old_page')][['user_id', 'group']]\n",
    "    # identify treatment group users exposed to control\n",
    "    df3 = pd.concat([df1, df2])\n",
    "    df4 = pd.merge(df, df3, on=['user_id'], how='left')\n",
    "    return df4[df4['group_y'].isna()][['user_id', 'time_stamp', 'group_x', 'anding_page', 'converted']]\\\n",
    ".rename(columns={'group_x', group})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db71dd3d",
   "metadata": {},
   "source": [
    "### Multiple data points\n",
    "#### whether one user_id has multiple lines, if yes, merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ee869b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_multiple_exposure(df):\n",
    "    df1 = df[['user_id', 'group']].groupby(['user_id']).count().reset_index()\n",
    "    df2 = df1[df1['gorup']>1]['user_id'].count()\n",
    "    print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74ea3656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_multiple_exposures(df):\n",
    "    df1 = df.groupby(['user_id', 'group', 'landing_page'])\\\n",
    "    .agg({'timestamp':['min', 'max'], 'converted':['count', 'sum']})\n",
    "    df1.columns = df1.columns.droplevel(0)\n",
    "    df2 = df1.reset_index()\n",
    "    df2['converted'] = df2.apply(lambda x: int(x['sum']>0), axis=1) # 1 if the user has one conversion or more\n",
    "    df2['conversion_rate'] = 1.0 * df2['sum'] / df2['count']\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f0b3c5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data3 \u001b[38;5;241m=\u001b[39m consolidate_multiple_exposures(\u001b[43mdata2\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data2' is not defined"
     ]
    }
   ],
   "source": [
    "data3 = consolidate_multiple_exposures(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf78703",
   "metadata": {},
   "source": [
    "#### User_id: unit of randomization\n",
    "#### a/b testing sampling iid\n",
    "#### unit of analysis is also on user_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dfada6",
   "metadata": {},
   "source": [
    "## A/B testing sanity check, find out whether bias exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0545f04d",
   "metadata": {},
   "source": [
    "### STEP4 sampling bias\n",
    "#### Goal: check if the sampling is random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "770c4fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 50% to control, 50% to treatment\n",
    "#### Check if treatment is 50%\n",
    "#### Use binomial test\n",
    "###### countries\n",
    "###### groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe1e9b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_any_assignment_imbalance(df):\n",
    "    df1 = df[['user_id', 'group']].groupby['group'].count().reset_index()\n",
    "    # count the no. of users in treament vs control\n",
    "    print(df1)\n",
    "    pvalue = stats.binom_test(df1[df1['group']=='treatment']['user_id'].values[0]), \n",
    "    n = df1['user_id'].sum(), \n",
    "    p=0.5, \n",
    "    alternative='two-sided'\n",
    "    print(pvalue)               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ffcb60",
   "metadata": {},
   "source": [
    "# SESSION 2: Start the experiment with statistical inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b84c006",
   "metadata": {},
   "source": [
    "### Null Hypothesis: the conversion rate at treatment group (\"new page\") is higher than the control group (\"old page\")\n",
    "#### Statistical inference\n",
    "#### p-value and confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa6d10e",
   "metadata": {},
   "source": [
    "#### Metrics: Delta = (treatment converted ==1 proportion) - (control converted ==1 proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbe2e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Population conversion is what the corp cares about\n",
    "### Sample conversion is partial data\n",
    "### Delta represent sample, but care about population\n",
    "### Population_conversion is a CONSTANT\n",
    "### Need to add a confidence interval to \"sample conversion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b49bfc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assume Delta follows a t-distribution. User population is 10k+, z-distribution/t-distribution difference is small\n",
    "### Not that important for the stingency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d67fce",
   "metadata": {},
   "source": [
    "### STEP1: Get the pooled standard error of delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b5958d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (2427705628.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [3]\u001b[0;36m\u001b[0m\n\u001b[0;31m    /(var_treatment**2)/(n_treatment**2*(n_treatment-1)) + var_control**2/(n_control**2*(n_control-1)))\u001b[0m\n\u001b[0m                                                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "def calculate_pvalue(df):\n",
    "    n_treatment = df[df['group'] == 'treatment']['user_id'].count()\n",
    "    n_control = df[df['group'] == 'control']['user_id'].count()\n",
    "    \n",
    "    p_treatment = 1.0*df[df['group'] == 'treatment']['converted'].sum()/n_treatment\n",
    "    # the probability of a user in treament to convert\n",
    "    p_control = 1.0*df[df['group'] == 'control']['converted'].sum()/n_control\n",
    "    \n",
    "    # \"converted\" is binary, simplify \"variance\" calculation equation to the proportion of users with converted==1\n",
    "    var_treatment = p_treatment * (1 - p_treatment)\n",
    "    var_control = p_control * (1-p_control)\n",
    "    \n",
    "    p_delta = p_treaemtn - p_control\n",
    "    print(p_delta)\n",
    "    \n",
    "    pooled_se = np.sqrt(var_treatment/n_treatment + var_control/n_control)\n",
    "    # t-distribution also needs degree of freedom\n",
    "    dof = (var_treatment/n_treatment + var_control/n_control)**2 \\\n",
    "    /(var_treatment**2)/(n_treatment**2*(n_treatment-1)) + var_control**2/(n_control**2*(n_control-1)))\n",
    "    \n",
    "    # the p vaooue of the t test\n",
    "    pvalue = 2*t.cdf(~abs(t_statistic), dof)\n",
    "    # pvalue equation for t-statistics:   p-value = t.cdf(x, df, loc=0, scale=1)\n",
    "    print (pvalue)\n",
    "    \n",
    "    # 95% CI\n",
    "    # ppf(Percent point function) (inverse of cdf — percentiles).\n",
    "    lower = p_delta - t.ppf(0.975, dof)*pooled_ss\n",
    "    upper = p_delta + t.ppf(0.975, dof)*pooled_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2b4f608",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m calculate_pvalue(\u001b[43mdf\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "calculate_pvalue(df)\n",
    "## if the confidence interval contains 0, then cannot reject Null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96975f3",
   "metadata": {},
   "source": [
    "### CI:  if the confidence interval contains 0, then cannot reject Null hypothesis\n",
    "\n",
    "### p-value: p-value of when conversion rate not impoacted by page changes (under null hypothesis), but the delta < -0.001 or > 0.001. 23%>>5% cannot reject Null hypothesis\n",
    "#### Two-sided t-test, need to add both sides (>0.001 <-0.001) together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db980b8e",
   "metadata": {},
   "source": [
    "# SESSION 3: Power calculation with python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f4533",
   "metadata": {},
   "source": [
    "### 3 important designs to support the experiment\n",
    "#### 1. What's the unit of randomization (user_id? page impression - each user may have to impressions)\n",
    "######## if use impression, may violate iid assumption\n",
    "#### 2. What's the metrics. Unit of randomization need to be consistent with unit of analysis \n",
    "######## if use_id: (metric: proportion of users who are converted to 1)\n",
    "######## if impression: (metric: proportion of impressions who are converted to 1)\n",
    "#### 3. Select sample size based on (1) alpha, (2) beta, (3) standard deviation and (4) minimum detectable effect\n",
    "##### alpha is 5%. 5% probability that when null hypothesis is true, still reject (Type I error)\n",
    "##### beta is 80% (Type II error). Alternative hyposis mean at miminum detectable effect, still reject No. \n",
    "##### The probability of making a type II error (failing to reject the null hypothesis when it is actually false) is called β (beta). The quantity (1 - β) is called power/sensitivity, the probability of observing an effect in the sample (if one), of a specified effect size or greater exists in the population.\n",
    "##### Minimum detectable effect (MDE) is a calculation that estimates the smallest improvement you are willing to be able to detect. It determines how \"sensitive\" an experiment is.\n",
    "\n",
    "#### Note: standard deviation is coming from data, combine control and treatment groups\n",
    "#### Sample size calculated before experiment, thus no control and treatment grouping. Pool all valid users together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ba201f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mde(df):\n",
    "    n_treatment = df[df['group'] == 'treatment']['user_id'].count()\n",
    "    n_control = df[df['group'] == 'control']['user_id'].count()\n",
    "    \n",
    "    power_analysis = smp.TTestIndPower()\n",
    "    \n",
    "    effect_size = power_analysis.solve_power(\n",
    "    nobs1=n_control, power=0.8, alpha=0.05, ratio=1.0*n_treatment/n_control, alternative=\"two_sided\")\n",
    "    p = 1.0*df['converted'].sum()/df['user_id'].count() # nobs = number of observations\n",
    "    sd = np.sqrt(p*(1-p)) # the standard deviation of a user to convert\n",
    "    mde = effect_size*sd # the minimum detectable effect with the with the current sample size\n",
    "    print(mde)\n",
    "    \n",
    "    p_treatment = 1.0*df[df['group'] == 'treatment']['converted'].sum()/n_treatment\n",
    "    # the probability of a user in treament to convert\n",
    "    p_control = 1.0*df[df['group'] == 'control']['converted'].sum()/n_control\n",
    "    # the probability of a user in control to convert\n",
    "    p_delta = p_treatment - p_control\n",
    "    nobs1 = power_analysis.solve_power(\n",
    "    effect_size=1.0 * p_delta/sd, power=0.8, alpha=0.05, ratio=1.0, alternative='two_sided')\n",
    "    # the required sample size when setting the currently measured delta as the minimum detectale effect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
